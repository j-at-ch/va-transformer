{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"name":"main.ipynb","provenance":[{"file_id":"1sHWQsk4xkg0oLNI62ZlscMdj0uCRFQqo","timestamp":1626188657832},{"file_id":"1GNps3iNQE_DJhzdE1RkCnqUIDlk_LGx2","timestamp":1625136080623}],"collapsed_sections":["Cg-gzA7tqjap","MVUY6qYzkdOu","bllR8BhQdfpH","YYCK4nwgWlSu"]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"dx4EQD3UfXjN"},"source":["## COLAB SETUP"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jTSRguDDUyL9","executionInfo":{"status":"ok","timestamp":1626191906919,"user_tz":-60,"elapsed":65983,"user":{"displayName":"James Cann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj37bBvXyKMzK890sou9cHG0PcfWI1S5-uihybe=s64","userId":"00456860292205766006"}},"outputId":"55aa161c-8032-4663-fd03-fc792633526b"},"source":["# mount your drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AfbwEJZ3gIX8","executionInfo":{"status":"ok","timestamp":1626192993056,"user_tz":-60,"elapsed":208,"user":{"displayName":"James Cann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj37bBvXyKMzK890sou9cHG0PcfWI1S5-uihybe=s64","userId":"00456860292205766006"}}},"source":["space = 'colab'"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"5FN2tXvggDta","executionInfo":{"status":"ok","timestamp":1626192994166,"user_tz":-60,"elapsed":209,"user":{"displayName":"James Cann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj37bBvXyKMzK890sou9cHG0PcfWI1S5-uihybe=s64","userId":"00456860292205766006"}}},"source":["if space == 'colab':\n","    data_root = '/content/drive/MyDrive/Data'\n","    save_root = ''\n","else:\n","    data_root = 'C:/Users/james/Data/MIMIC/mimic-iii-clinical-database-1.4'\n","    save_root = 'C:/Users/james/Data/MIMIC/mimic-iii-chart-transformers'"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"cmfULdfNVGDk"},"source":["!pip install x_transformers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZiNstdm5rLcI"},"source":["## TENSORBOARD UTILS"]},{"cell_type":"code","metadata":{"id":"5RPe_7XfrPE3"},"source":["%reload_ext tensorboard"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HQIzy2pj3E9L"},"source":["from torch.utils.tensorboard import SummaryWriter\n","#writer = SummaryWriter()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A9x-ZD4wLE3G"},"source":["tensorboard --logdir runs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cg-gzA7tqjap"},"source":["## PRE-PROCESSING"]},{"cell_type":"code","metadata":{"id":"iyvdkWonrunP"},"source":["import os\n","import numpy as np\n","import pandas as pd\n","import pickle as pickle\n","import torch\n","from sklearn.model_selection import train_test_split\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6YBDVsAsryy0"},"source":["# paths\n","\n","chartevents_path = os.path.join(data_root, \"CHARTEVENTS.csv\")\n","admissions_path = os.path.join(data_root,\"ADMISSIONS.csv\")\n","d_items_path = os.path.join(data_root, \"d_items.csv\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r_FdSrcoqa88"},"source":["# read in admissions\n","\n","admissions = pd.read_csv(admissions_path,\n","                         parse_dates=['ADMITTIME', 'DISCHTIME'])\n","\n","# extract only those charted and apply labelling logic\n","\n","charted = admissions[admissions.HAS_CHARTEVENTS_DATA == 1]\n","charted.drop('ROW_ID', axis=1, inplace=True)\n","charted['HADM_IN_SEQ'] = charted.groupby('SUBJECT_ID')['ADMITTIME'].rank().astype(int)\n","charted = charted.sort_values(by=['SUBJECT_ID', 'HADM_IN_SEQ'])\n","charted['ADMITTIME_NEXT'] = charted.groupby('SUBJECT_ID')['ADMITTIME'].shift(-1)\n","charted['DIS2ADM'] = charted['ADMITTIME_NEXT'] - charted['DISCHTIME']\n","charted['READM<7'] = (charted['DIS2ADM'] < pd.Timedelta(days=7)).astype(int)\n","charted['READM<30'] = (charted['DIS2ADM'] < pd.Timedelta(days=30)).astype(int)\n","charted.set_index('HADM_ID', inplace=True)\n","\n","# get hadm_ids for the first admission\n","\n","first_indices = charted[charted.HADM_IN_SEQ == 1].index.to_numpy()\n","\n","# split first-hadm_ids into train, val, test and check.\n","\n","train_indices, surplus = train_test_split(first_indices, train_size=0.8)\n","val_indices, test_indices = train_test_split(surplus, test_size=0.5)\n","del surplus\n","assert set(first_indices) == set(train_indices) | set(val_indices) | set(test_indices)\n","\n","# helpers\n","\n","\n","def ts_to_posix(time):\n","    return pd.Timestamp(time, unit='s').timestamp()\n","\n","\n","def get_admittime(hadm_id):\n","    time = charted.loc[hadm_id, 'ADMITTIME']\n","    return ts_to_posix(time)\n","\n","\n","def get_from_charted(hadm_id, label):\n","    return charted.loc[hadm_id, label]\n","\n","\n","# token mappings\n","\n","d_items = pd.read_csv(d_items_path)\n","\n","token_shift = 1\n","pad_token = 0\n","\n","itemid2token = dict(zip(d_items['ITEMID'], range(token_shift, token_shift + len(d_items))))\n","\n","# add special tokens to the dictionary\n","itemid2token['[PAD]'] = pad_token\n","#itemid2token['[BOS]'] = 1\n","#itemid2token['[EOS]'] = 2\n","#itemid2token['[SEP]'] = 3\n","\n","token2itemid = {v: k for k, v in itemid2token.items()}\n","token2label = dict(zip(range(len(d_items)), d_items['LABEL']))\n","\n","with open(os.path.join(save_root, 'mappings.pkl'), 'wb') as f:\n","    pickle.dump({'itemid2token': itemid2token,\n","                 'token2itemid': token2itemid},\n","                f)\n","\n","\n","def map2token(itemid):\n","    return itemid2token[np.int(itemid)]\n","\n","\n","def map2itemid(token):\n","    return str(token2itemid[token])\n","\n","\n","def map2itemidstr(tokens):\n","    return ' '.join(list(map(map2itemid, tokens)))\n","\n","\n","# loop through sets and generate output files\n","\n","for subset in ['val', 'train', 'test']:\n","    print(f'Processing {subset} set data...')\n","\n","    # grouper for charts\n","\n","    gpdf = (pd.read_csv(chartevents_path, skiprows=0, \n","                        nrows=10000000 if space != 'colab',\n","                        header=0,\n","                        usecols=['HADM_ID', 'CHARTTIME', 'ITEMID'],\n","                        dtype={'HADM_ID': np.int},\n","                        converters={'ITEMID': map2token},\n","                        parse_dates=['CHARTTIME'])\n","            .query(f'HADM_ID.isin(@{subset}_indices)')\n","            .groupby(by='HADM_ID')\n","            )\n","\n","    # initialise\n","\n","    tokens = dict()\n","    times = dict()\n","    times_rel = dict()\n","    labels = dict()\n","\n","    # populate with entries\n","\n","    for i in gpdf.groups:\n","        time_origin = get_admittime(i)\n","        temp = gpdf.get_group(i).sort_values(by=\"CHARTTIME\")\n","        tokens[i] = np.array(temp['ITEMID'], dtype=int)\n","        times[i] = np.fromiter(\n","            map(ts_to_posix, temp['CHARTTIME']),\n","            dtype=np.int64\n","        )\n","        times_rel[i] = times[i] - time_origin\n","        labels[i] = {\n","            'readm_7': get_from_charted(i, 'READM<7'),\n","            'readm_30': get_from_charted(i, 'READM<30')\n","        }\n","\n","    # write out charts to pickle\n","\n","    save_path = os.path.join(save_root, f'{subset}_charts.pkl')\n","\n","    with open(save_path, 'wb') as f:\n","        pickle.dump({f'{subset}_tokens': tokens,\n","                     f'{subset}_times': times,\n","                     f'{subset}_times_rel': times_rel}, f)\n","\n","    del tokens, times, times_rel, gpdf\n","\n","    # write out labels to pickle\n","\n","    save_path = os.path.join(save_root, f'{subset}_labels.pkl')\n","\n","    with open(save_path, 'wb') as f:\n","        pickle.dump({f'{subset}_labels': labels}, f)\n","\n","    del labels\n","\n","    print(f'{subset} set data processed!')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eAUowrZpu3ed"},"source":["## SELF-SUPERVISED MODE\n","\n"]},{"cell_type":"code","metadata":{"id":"WUjIH_4q_gzL"},"source":["import os\n","import copy\n","import tqdm\n","import random\n","\n","import numpy as np\n","import pandas as pd\n","import pickle as pickle\n","import torch\n","\n","from torch.utils.data import DataLoader, Dataset\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from x_transformers import TransformerWrapper, Decoder\n","from x_transformers.autoregressive_wrapper import AutoregressiveWrapper"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0YD8BurHdbDu"},"source":["### THE MODEL"]},{"cell_type":"markdown","metadata":{"id":"myB-nKwoVmWe"},"source":["#### Mappings, Paths and Utils"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"RemqYJeTu2s7"},"source":["# paths\n","\n","train_path = os.path.join(data_root, \"train_charts.pkl\")\n","val_path = os.path.join(data_root, \"val_charts.pkl\")\n","mapping_path = os.path.join(data_root, \"mappings.pkl\")\n","\n","# misc\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# token mappings:  # TODO: refactor to module where possible.\n","\n","with open(mapping_path, 'rb') as f:\n","    mappings = pickle.load(f)\n","    itemid2token = mappings['itemid2token']\n","    token2itemid = mappings['token2itemid']\n","    del mappings\n","\n","num_tokens = len(itemid2token)\n","\n","# token mappings: decoders\n","\n","\n","def decode_token(token):\n","    return str(token2itemid[token])\n","\n","\n","def decode_tokens(tokens):\n","    return ' '.join(list(map(decode_token, tokens)))\n","\n","\n","# get data\n","\n","def fetch_data(path, var_key):\n","    with open(path, 'rb') as f:\n","        data = pickle.load(f)\n","    return data[var_key]\n","\n","\n","trX = fetch_data(train_path, 'train_tokens')\n","vaX = fetch_data(val_path, 'val_tokens')\n","\n","data_train = {k: torch.from_numpy(v) for k, v in trX.items()}\n","data_val = {k: torch.from_numpy(v) for k, v in vaX.items()}\n","\n","\n","# yield from loader\n","\n","def cycle(loader):\n","    while True:\n","        for data in loader:\n","            yield data\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FgHrfl-GVe0l"},"source":["#### Constants & Model"]},{"cell_type":"code","metadata":{"id":"6GVoPb-bVdw5"},"source":["# constants & hyperparameters \n","\n","NUM_EPOCHS = 3\n","NUM_BATCHES = 10\n","BATCH_SIZE = 4\n","GRADIENT_ACCUMULATE_EVERY = 4  # 4\n","LEARNING_RATE = 1e-4\n","VALIDATE_EVERY = 10\n","CHECKPOINT_AFTER = 10\n","GENERATE_EVERY = 100\n","GENERATE_LENGTH = 100\n","SEQ_LEN = 200\n","\n","# instantiate GPT-like decoder model\n","\n","\n","LAYER_SPEC = {'dim':100, 'depth':3, 'heads':4}  # full: 512, 6, 8\n","\n","model = TransformerWrapper(\n","    num_tokens=num_tokens,  # Expects each val in data to be [0, num_tokens)\n","    max_seq_len=SEQ_LEN, \n","    attn_layers=Decoder(\n","        dim=LAYER_SPEC['dim'],\n","        depth=LAYER_SPEC['depth'],\n","        heads=LAYER_SPEC['heads'])\n",")\n","\n","pre_model = AutoregressiveWrapper(model)\n","pre_model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IIMyYHwrUxST"},"source":["#### Datasets & Dataloaders"]},{"cell_type":"code","metadata":{"id":"fwNfiD4tUwFe"},"source":["class ClsSamplerDataset(Dataset):  # TODO: tidy __getitem__ method with more natural pad operations.\n","    def __init__(self, data, seq_len, labels=None):\n","        super().__init__()\n","        self.data = data\n","        self.labels = labels\n","        self.seq_len = seq_len\n","        self.lookup = dict(zip(np.arange(len(self.data)), self.data.keys()))\n","\n","    def __getitem__(self, key):  # a.t.m. when data[key] shorter length than SEQ_LEN, padded with 0.\n","        index = self.lookup[key]\n","        item_len = self.data[index].size(0)\n","        rand_start = torch.randint(0, item_len - self.seq_len, (1,)) if item_len > self.seq_len else 0\n","        lenfromseq = min(item_len, self.seq_len)\n","        sample = torch.zeros(self.seq_len)\n","        sample[:lenfromseq] = self.data[index][rand_start: rand_start + lenfromseq]\n","\n","        if self.labels is not None:\n","            label = torch.tensor(self.labels[index])\n","            return sample.long().to(device), label.long().to(device)\n","        else:\n","            return sample.long().to(device)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","\n","train_dataset = ClsSamplerDataset(data_train, SEQ_LEN)\n","val_dataset   = ClsSamplerDataset(data_val, SEQ_LEN)\n","\n","train_loader  = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n","val_loader    = DataLoader(val_dataset,   batch_size=BATCH_SIZE)\n","\n","train_cycler  = cycle(train_loader)\n","val_cycler    = cycle(val_loader)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qoyiGyrsfZ29"},"source":["### TRAINING LOOP"]},{"cell_type":"code","metadata":{"id":"_4s0dfbsfW3f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626179988403,"user_tz":-60,"elapsed":9157,"user":{"displayName":"James Cann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj37bBvXyKMzK890sou9cHG0PcfWI1S5-uihybe=s64","userId":"00456860292205766006"}},"outputId":"3c5cdff1-7e92-46e1-96b9-105a2c99f809"},"source":["optim = torch.optim.Adam(pre_model.parameters(), lr=LEARNING_RATE)\n","ckpt_path = os.path.join(save_root, \"pre_model_exp1.pt\")\n","\n","writer = SummaryWriter(\n","    log_dir=\"runs/pre_model\",\n","    filename_suffix='_' + '_'.join(map(str, LAYER_SPEC.values()))\n","    )\n","\n","# training loop\n","\n","best_val_loss = np.inf\n","\n","for epoch in range(NUM_EPOCHS):\n","  for i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10.,\n","                     desc=f'epoch {epoch}:'):\n","      pre_model.train()\n","\n","      for __ in range(GRADIENT_ACCUMULATE_EVERY):\n","          loss = pre_model(next(train_cycler))\n","          loss.backward()\n","\n","      torch.nn.utils.clip_grad_norm_(pre_model.parameters(), 0.5)\n","      optim.step()\n","      optim.zero_grad()\n","\n","      # parameter tracking\n","\n","      writer.add_scalar('train_loss', loss.item(),\n","                        epoch * NUM_BATCHES + i\n","                        )\n","\n","      # validate model\n","\n","      if i % VALIDATE_EVERY == 0:\n","          pre_model.eval()\n","          with torch.no_grad():\n","              val_loss = pre_model(next(val_cycler)).item()\n","              \n","              writer.add_scalar('val_loss', val_loss,\n","                        epoch * NUM_BATCHES + i\n","                        )\n","\n","              if val_loss < best_val_loss:\n","                  print(f'VL: {val_loss} < BVL: {best_val_loss}')\n","                  best_val_loss = val_loss\n","\n","                  # checkpoint model\n","\n","                  if i + epoch * NUM_BATCHES > CHECKPOINT_AFTER:\n","                    print(\"Checkpoint saving...\")\n","                    torch.save({\n","                        'epoch': epoch,\n","                        'train_step': i,\n","                        'model_state_dict': pre_model.state_dict(),\n","                        'LAYER_SPEC': LAYER_SPEC,\n","                        'SEQ_LEN': SEQ_LEN,\n","                        'optim_state_dict': optim.state_dict(),\n","                        'val_loss': val_loss\n","                    }, ckpt_path)\n","                    print(\"Checkpoint saved!\\n\")\n","      \n","      # generate sequence\n","\n","      if i % GENERATE_EVERY == 0:\n","          pre_model.eval()\n","          inp = random.choice(val_dataset)[:-1]\n","          primer_str = decode_tokens(inp.cpu().numpy())\n","          print('\\nprimer:', primer_str, '*' * 100, sep='\\n')\n","\n","          sample = pre_model.generate(inp, GENERATE_LENGTH)\n","          sample_str = decode_tokens(sample.cpu().numpy())\n","          print('output:', sample_str, '\\n', sep='\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","epoch 0::   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["VL: 9.234357833862305 < BVL: inf\n","\n","primer:\n","227443 220602 220615 220621 220635 220645 225624 225625 225634 225651 225677 227073 227442 227429 220045 220210 220277 220179 220180 220181 220210 220277 220181 220045 220179 220180 220051 220050 220052 220045 220050 220051 220052 220179 220180 220181 220210 223761 220277 225698 226534 226536 224828 225668 223830 227464 220224 226537 220235 220277 220210 220052 220051 220045 220050 220210 220277 220052 220051 220045 220050 223834 223835 220210 223834 220045 220050 220051 220052 223835 220277 220045 220050 220051 220210 220052 220277 220052 220210 220277 220050 220051 220045 220045 220050 220051 220052 220210 220277 220210 220277 220051 220050 220052 220045 220210 220045 220050 220051 220052 220277 220210 220052 220051 220050 220277 220045 220045 220050 220277 220051 220210 220052 220045 220210 220052 220277 220050 220051 220045 220050 220051 220052 220210 220277 220051 220045 220210 220052 220277 220050 220045 220050 220051 220052 220210 220277 220046 220056 220210 220277 223751 223752 223761 223769 223770 223834 223835 224161 224162 226253 220058 220052 220051 220050 220047 220045 220052 220045 220210 220277 220051 220050 220045 220050 220051 220052 220210 220277 220210 220045 220050 220051 220277 220052 225698 223830 224828 220224 220235 224697 224688 224689 224690 224695 224687 224696 224738 226873 225624 225690 226871 227073 227442 227443 227456 224685 225612 224684\n","****************************************************************************************************\n","output:\n","224442 6931 46361 6908 274 226105 225937 80306 223761 42941 8421 42840 80056 44881 42239 225091 46708 1032 5824 6112 225217 224453 8139 6063 4729 45785 44132 46682 226998 40473 40385 225489 42051 43386 41219 618 223851 30045 226037 584 226129 42998 228229 223873 5690 3350 45398 226037 8508 225091 227980 42337 2836 72 44232 470 43108 225937 5808 8473 41451 2556 228388 8451 226610 227121 5596 220541 45310 2481 225321 1650 6229 227056 227360 775 3592 7094 224796 6021 224337 1036 4707 1354 7798 42245 45000 1939 223769 8572 2079 30118 5077 30125 42026 5690 4184 382 6171 80218\n","\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\repoch 0:: 100%|██████████| 10/10 [00:02<00:00,  3.35it/s]\n","\n","epoch 1::   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["VL: 8.518343925476074 < BVL: 9.234357833862305\n","\n","primer:\n","223761 220179 220045 220180 220210 220277 220181 220047 223751 224162 223752 223769 223770 224161 226253 220046 220277 220045 220210 220277 220179 220045 220210 220181 220180 220045 220179 220180 220181 220210 220277 223761 220045 220277 220210 226531 226512 226707 226730 220179 220180 220181 220210 220277 220045 220181 220045 220179 220180 220277 220210 220277 220210 220181 220180 220179 220045 220180 220179 220045 223761 220277 220181 220210 226253 224161 223770 223769 223752 223751 220047 220046 224162 220632 227465 227457 227456 227445 227443 227442 227429 227073 225690 225677 225634 225625 225624 225612 220645 220644 220228 220635 220277 220545 220546 220587 220602 220615 220621 227466 227467 220210 220181 220045 220179 220180 220277 220045 220179 220180 220181 220210 220180 220181 220045 220179 220277 220210 220046 220045 220047 220179 220180 220181 220210 223769 223751 223752 223761 223770 224161 224162 220277 220045 220179 220180 220181 220210 220277 220180 220179 220181 220210 220277 225664 220045 220045 220179 220180 220181 220210 220277 220181 220179 220210 220045 220180 220277 220277 220210 220181 220180 220179 220045 220545 220180 220277 220045 220179 220181 220210 220277 220210 220181 220179 220045 220180 220180 220045 220046 220047 220179 220210 220181 223751 223752 223761 223769 223770 224161 224162 225664 220277 220045 220179 220180 220181 220210 220277\n","****************************************************************************************************\n","output:\n","7276 80209 227374 228328 5869 224666 80020 295 228327 30053 80027 1570 916 5791 220069 43115 227016 3455 4769 220181 43356 2819 82 42772 7647 42914 5871 1614 6179 41049 228628 7402 1032 228380 1216 228526 7838 80261 6179 42943 1127 226129 45520 7328 188 44800 226998 4462 41122 8320 6231 43622 42793 2815 45163 4858 43115 2563 226041 43049 4243 7286 46317 80037 43088 45910 6081 44165 30033 226037 41897 7287 45166 42216 45447 7500 227983 45315 42942 42092 1504 42631 1737 46789 44080 2434 2563 228388 221 70075 223962 2836 44494 45114 40635 7808 225818 4412 45303 32\n","\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\repoch 1:: 100%|██████████| 10/10 [00:02<00:00,  3.49it/s]\n","\n","epoch 2::   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["VL: 8.259032249450684 < BVL: 8.518343925476074\n","Checkpoint saving...\n","Checkpoint saved!\n","\n","\n","primer:\n","220587 220602 220644 220621 220632 220635 220227 226537 225698 225667 224828 223830 220224 220235 226531 220210 220045 220050 220051 220052 220277 225664 227187 225664 220339 220277 220051 220052 220050 220045 220210 227187 227566 227565 226873 224690 220051 220052 220210 220277 220292 220293 220339 223835 223873 223874 223875 223876 224417 224684 224685 224686 224687 224688 224689 224695 224697 224738 225664 220050 226871 220045 224421 224422 224828 220224 220227 220235 223830 225698 220051 220050 220047 225664 220045 220052 224697 220046 226253 220056 220210 224695 224689 220058 224686 224685 224687 224161 223770 223769 220339 224162 220277 223834 223835 225664 220051 220050 220052 220045 220210 220227 220277 223761 225664 220052 220210 220277 223834 220050 220051 220045 220045 220210 220052 220050 220051 220277 227467 227466 227465 227429 227445 225634 220277 220045 220050 220052 220051 220210 225664 223834 220050 220210 220052 220051 220045 220277 220050 220051 220052 220210 220277 220045 220277 220045 220050 220051 220052 220210 223761 220210 220052 220277 220045 220051 220050 220045 220050 220051 220052 220210 220277 220045 220050 220051 220210 220052 220277 220045 220050 220051 220052 220210 220277 225664 220277 220210 220050 220045 220052 220051 220228 220545 220546 220581 220602 220587 227467 227466 227465 227457 227456 227443 227073 225690 225677 227442 225625\n","****************************************************************************************************\n","output:\n","90019 3625 6414 2406 4887 223994 5800 227046 43122 4407 227947 3058 45041 4646 811 46307 5759 42914 4820 2293 228066 44265 6029 220180 8159 3074 42779 225450 2563 42793 225339 7737 46544 5928 42530 42981 7520 4820 26 1853 228362 639 42453 227789 40388 1341 40848 3326 220047 3810 228647 30348 1515 46469 1113 4855 30373 5562 227046 46117 42986 7003 4304 225869 225625 5776 220965 70083 224135 228071 220061 1532 688 42091 226369 223946 404 44127 1361 5096 712 6934 30008 6838 30118 44419 227465 8390 4646 44468 42278 5729 2434 46197 1378 227812 226618 227963 30209 225954\n","\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\repoch 2:: 100%|██████████| 10/10 [00:02<00:00,  3.37it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"MVUY6qYzkdOu"},"source":["### EVALUATE"]},{"cell_type":"code","metadata":{"id":"Tm9Q6o0zkcfp"},"source":["@torch.no_grad()\n","def evaluate(model, dataloader):\n","    model.eval()\n","    cum_loss = 0\n","    counter = 0\n","    for batch in dataloader:\n","        counter += 1\n","        batch_size = batch.shape[0]\n","        val_loss = model(batch).item()\n","        cum_loss += val_loss\n","    avg_loss = cum_loss/(batch_size*counter)\n","    return cum_loss, avg_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QNTfwTDxlNgA","executionInfo":{"status":"ok","timestamp":1625506895767,"user_tz":-60,"elapsed":93489,"user":{"displayName":"James Cann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj37bBvXyKMzK890sou9cHG0PcfWI1S5-uihybe=s64","userId":"00456860292205766006"}},"outputId":"9233365e-9b4b-4492-ebfa-b1aa88e181be"},"source":["evaluate(pre_model, val_loader)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2384.7820653915405, 2.3943595034051612)"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"L9kSueJZWbW9"},"source":["Will this code work for finetuning too?"]},{"cell_type":"markdown","metadata":{"id":"bllR8BhQdfpH"},"source":["### GENERATING SEQUENCES"]},{"cell_type":"code","metadata":{"id":"pqGhUlnRGWcM"},"source":["# reading d_items for interpretability \n","d_items_path = os.path.join(data_root, \"d_items.csv\")\n","d_items = pd.read_csv(d_items_path, index_col='ITEMID', dtype={'ITEMID': str})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H3gZplilK-EL"},"source":["def decode_token(token):\n","    return str(token2itemid[token])\n","\n","def decode_tokens(tokens):\n","    return ' '.join(list(map(decode_token, tokens)))\n","\n","def token2label(token):\n","    if token == 0:\n","        return '[PAD]'\n","    else:\n","        itemid = token2itemid[token]\n","        x = d_items.loc[itemid, 'LABEL']\n","    return x\n","\n","def tokens2labels(tokens):\n","    return '\\n\\t -> '.join(list(map(token2label, tokens)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cizrIjWVdwIX"},"source":["# fetch and load model state_dict\n","\n","weights_path = os.path.join(data_root, 'models', 'pre_model_exp1.pt')\n","X = torch.load(weights_path, map_location=device)\n","states = X['model_state_dict']\n","base_states = { k[len('net.'):] if k[:len('net.')] == 'net.' else k : v for k, v in states.items()}\n","\n","pre_model.load_state_dict(states)\n","pre_model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xjk16p9KeyUv"},"source":["pre_model.eval()\n","with torch.no_grad():\n","    prompt = random.choice(val_dataset)[0:18]\n","    #prompt = torch.cat((prompt, torch.tensor([0]).to(device)))\n","\n","    sample = pre_model.generate(start_tokens=prompt, seq_len = 50, eos_token=0)\n","    print(\"prompt:\\t\", tokens2labels(prompt.cpu().numpy()))\n","    print(\"model:\\t\", tokens2labels(sample.cpu().numpy()), '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Da2PT5WVaEBf","executionInfo":{"status":"ok","timestamp":1625568213097,"user_tz":-60,"elapsed":35242,"user":{"displayName":"James Cann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj37bBvXyKMzK890sou9cHG0PcfWI1S5-uihybe=s64","userId":"00456860292205766006"}},"outputId":"e7b4b0a2-582e-42ad-bc43-2931dea599ff"},"source":["evaluate(pre_model, train_loader)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3122.3268125355244, 1.5635086692716698)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"YYCK4nwgWlSu"},"source":["### TESTING - DO NOT USE"]},{"cell_type":"code","metadata":{"id":"w47h1Q6_Wnz_"},"source":["test_path    = os.path.join(data_root, 'test_charts.pkl')\n","tsX          = fetch_data(test_path, 'test_tokens')\n","data_test    = {k: torch.from_numpy(v) for k, v in tsX.items()}\n","test_dataset = ClsSamplerDataset(data_test, SEQ_LEN)\n","test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n","test_cycler  = cycle(test_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wVDVCWE8vnWZ"},"source":["## FINE-TUNING MODE"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"M4oE0duaUQIJ"},"source":["class FinetuningWrapper(nn.Module):\n","    def __init__(self, net, num_classes, state_dict = None,\n","                 ignore_index = -100, pad_value = 0, weight = None):\n","        super().__init__()\n","        self.pad_value = pad_value\n","        self.ignore_index = ignore_index\n","        self.num_classes = num_classes\n","        self.weight = weight.to(torch.float) if weight is not None else weight \n","          # expected to be a float tensor of size = num_classes\n","\n","        self.net = copy.deepcopy(net)  # deepcopy is necessary here.\n","        self.max_seq_len = self.net.max_seq_len\n","\n","        # initialise net from pretrained\n","        if state_dict is not None:\n","            self.net.load_state_dict(state_dict)\n","\n","        # define classifier head layers\n","        self.num_features = net.to_logits.in_features * 200\n","        self.net.clf1 = nn.Linear(self.num_features, num_classes, bias=True)\n","\n","    def forward(self, X, Y, predict=False, **kwargs):\n","        Z = self.net(X, return_embeddings=True, **kwargs)\n","        Z = torch.flatten(Z, start_dim=1)\n","        logits = self.net.clf1(Z)\n","        loss = F.cross_entropy(logits, Y, weight = self.weight)\n","        return logits if predict else loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"5a_vWVFHUQIM"},"source":["train_lbl_path = os.path.join(data_root, \"train_labels.pkl\")\n","val_lbl_path = os.path.join(data_root, \"val_labels.pkl\")\n","FT_BATCH_SIZE = 100\n","\n","# fetch labels\n","\n","with open(train_lbl_path, 'rb') as f:\n","    X = pickle.load(f)\n","    train_labels_30 = {k: v['readm_30'] for k, v in  X['train_labels'].items()}\n","    train_labels_7 = {k: v['readm_7'] for k, v in  X['train_labels'].items()}\n","    del X\n","\n","with open(val_lbl_path, 'rb') as f:\n","    X = pickle.load(f)\n","    val_labels_30 = {k: v['readm_30'] for k, v in  X['val_labels'].items()}\n","    val_labels_7 = {k: v['readm_7'] for k, v in  X['val_labels'].items()}\n","    del X\n","\n","# helper for propensities\n","\n","def propensity(di):\n","    x = sum(di.values()) / len(di)\n","    return x\n","\n","# generate datasets and loaders\n","\n","ft_train_dataset = ClsSamplerDataset(data_train, SEQ_LEN, labels=train_labels_30)\n","ft_val_dataset = ClsSamplerDataset(data_val, SEQ_LEN, labels=val_labels_30)\n","\n","ft_train_loader = cycle(DataLoader(ft_train_dataset, batch_size=FT_BATCH_SIZE))\n","ft_val_loader = cycle(DataLoader(ft_val_dataset, batch_size=FT_BATCH_SIZE))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"ggZs5eGAUQIN"},"source":["# fetch model weights\n","\n","params_path = os.path.join(data_root, 'models', 'pre_model_exp1.pt')\n","X = torch.load(params_path, map_location=device)\n","states = X['model_state_dict']\n","base_states = { k[len('net.'):] if k[:len('net.')] == 'net.' else k : v for k, v in states.items()}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"zc4WuVlZUQIN"},"source":["### FINETUNING LOOP"]},{"cell_type":"markdown","metadata":{"id":"h_ey6WEKcmEO"},"source":["#### TRAINING"]},{"cell_type":"code","metadata":{"id":"TnFlV5e-l6DS"},"source":["# propensities\n","\n","p = propensity(train_labels_30)\n","weights = torch.tensor([p, 1-p]).to(device)\n","\n","# initialisation\n","\n","fit_model = FinetuningWrapper(model, num_classes=2,\n","                              state_dict=base_states,\n","                              weight=weights)\n","fit_model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"1uqkNxB7UQIN"},"source":["logs_path = os.path.join(save_root, 'logs', 'fit_weights')\n","NUM_FT_BATCHES = 1000\n","optim_ft = torch.optim.Adam(fit_model.parameters(), lr=0.001)\n","ckpt_ft_path = os.path.join(save_root, 'fit_model_weights')\n","\n","writer = SummaryWriter(logs_path)\n","writer2 = SummaryWriter(logs_path)\n","\n","# training loop\n","\n","best_val_loss = np.inf\n","for i in tqdm.tqdm(range(NUM_FT_BATCHES), mininterval=10., desc='fine-tuning'):\n","    fit_model.train()\n","\n","    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n","        X, Y = next(ft_train_loader)\n","        loss = fit_model(X, Y)\n","        loss.backward()\n","      \n","    writer.add_scalar('loss', loss.item(), i)\n","\n","    print(f'tuning loss: {loss.item()}')\n","    torch.nn.utils.clip_grad_norm_(fit_model.parameters(), 0.5)\n","    optim_ft.step()\n","    optim_ft.zero_grad()\n","\n","    # validate fit_model\n","\n","    if i % VALIDATE_EVERY == 0:\n","        fit_model.eval()\n","        with torch.no_grad():\n","            X, Y = next(ft_val_loader)\n","            val_loss = fit_model(X, Y).item()\n","            writer2.add_scalar('val_loss', val_loss, i)\n","            print(f'validation loss: {val_loss}')\n","            \n","            if val_loss < best_val_loss:\n","                best_val_loss = val_loss\n","                \n","                if i > CHECKPOINT_AFTER:\n","                    print(\"Saving checkpoint...\\n\")\n","                    torch.save({\n","                        'train_step': i,\n","                        'model_state_dict': fit_model.state_dict(),\n","                        'SEQ_LEN': SEQ_LEN,\n","                        'optim_state_dict': optim_ft.state_dict(),\n","                        'val_loss': val_loss\n","                    }, ckpt_ft_path)\n","                    print(\"Checkpoint saved!\\n\")\n","    #else:\n","    #    val_loss = np.nan\n","    #writer.add_scalars('loss', {'tuning_loss': loss.item(), 'val_loss': val_loss}, i)\n","\n","writer.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-qev_muUEQYJ"},"source":["#### PREDICTION"]},{"cell_type":"markdown","metadata":{"id":"UTNiHurMGwms"},"source":["Weight loading regimes:\n","\n","1. Initialise with random\n","2. Unsupervised pretrain\n","3. Load from `pretrain`\n","4. Finetune for `task`\n","5. Load from `finetune`\n","\n","Here, we are in 5.\n"]},{"cell_type":"code","metadata":{"id":"EsvMPvVdg_fb"},"source":["from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kbQEtxYLE8w1"},"source":["# fetch model weights\n","\n","#ft_model_path = os.path.join(data_root, 'models', 'fit_model_exp1.pt')\n","ft_model_path = os.path.join(data_root, 'models', 'ft_model_exp1')\n","X = torch.load(ft_model_path, map_location=device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2vB4rO9bEfKF"},"source":["ft_states = X['model_state_dict']\n","ft_base_states = { k[len('net.'):] if k[:len('net.')] == 'net.' else k : v for k, v in ft_states.items()}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZsabWGW7JBtb"},"source":["# initialisation\n","fit_model = FinetuningWrapper(model, num_classes=2)\n","fit_model.load_state_dict(ft_states)\n","fit_model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rykUxodGEPa5"},"source":["fit_model.eval()\n","\n","# nums TP, FP, TN, FN\n","\n","TP_tot = 0\n","FP_tot = 0\n","TN_tot = 0\n","FN_tot = 0\n","\n","with torch.no_grad():\n","    for i in range(10):\n","        X, Y_true = next(ft_val_loader)\n","        logits = fit_model(X, Y_true, predict=True)\n","        Y_pred = torch.argmax(logits, dim=1)\n","        y_true, y_pred = Y_true.cpu(), Y_pred.cpu()\n","        TP = ((y_true == 1) & (y_pred == 1)).sum()\n","        FN = (y_true > y_pred).sum()\n","        FP = (y_true < y_pred).sum()\n","        TN = ((y_true == 0) & (y_pred == 0)).sum()\n","        TP_tot += TP; FP_tot += FP; FN_tot += FN; TN_tot += TN; \n","        accuracy = (TP + TN) / (TP + TN + FP + FN)\n","        print(f'TP = {TP}', f'FP = {FP}', f'TN = {TN}', f'FN = {FN}')\n","        #print(f'TP: {Y_true.numpy().sum()}', f'pP: {Y_pred.numpy().sum()}')\n","        print('acc:', accuracy_score(y_true, y_pred, normalize=True),)\n","        print('bal_acc:', balanced_accuracy_score(y_true, y_pred),)\n","        print('conf:', '\\n', confusion_matrix(y_true, y_pred), '\\n')\n","\n","print(f'TP = {TP_tot}', f'FP = {FP_tot}', f'TN = {TN_tot}', f'FN = {FN_tot}')\n","spec = TN_tot / (TN_tot + FP_tot)\n","sens = TP_tot / (TP_tot + FN_tot)\n","print(f'spec_tot = {spec}', f'sens_tot = {sens}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xEAKF8AE8r6C"},"source":["tensorboard --logdir drive/MyDrive/Data/logs"],"execution_count":null,"outputs":[]}]}