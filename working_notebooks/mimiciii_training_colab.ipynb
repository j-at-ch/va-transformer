{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"name":"mimiciii_training_colab.ipynb","provenance":[{"file_id":"1GNps3iNQE_DJhzdE1RkCnqUIDlk_LGx2","timestamp":1625136080623}],"collapsed_sections":["ZiNstdm5rLcI","Cg-gzA7tqjap","FgHrfl-GVe0l","IIMyYHwrUxST","MVUY6qYzkdOu","bllR8BhQdfpH","YYCK4nwgWlSu"]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"dx4EQD3UfXjN"},"source":["## COLAB SETUP"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jTSRguDDUyL9","executionInfo":{"status":"ok","timestamp":1625567695485,"user_tz":-60,"elapsed":23397,"user":{"displayName":"James Cann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj37bBvXyKMzK890sou9cHG0PcfWI1S5-uihybe=s64","userId":"00456860292205766006"}},"outputId":"1be915a6-dfe6-4147-c1df-6c524f22866a"},"source":["# mount your drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AfbwEJZ3gIX8","executionInfo":{"status":"ok","timestamp":1625567698608,"user_tz":-60,"elapsed":201,"user":{"displayName":"James Cann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj37bBvXyKMzK890sou9cHG0PcfWI1S5-uihybe=s64","userId":"00456860292205766006"}}},"source":["space = 'colab'"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"5FN2tXvggDta","executionInfo":{"status":"ok","timestamp":1625567700057,"user_tz":-60,"elapsed":240,"user":{"displayName":"James Cann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj37bBvXyKMzK890sou9cHG0PcfWI1S5-uihybe=s64","userId":"00456860292205766006"}}},"source":["if space == 'colab':\n","    data_root = '/content/drive/MyDrive/Data'\n","    save_root = ''\n","else:\n","    data_root = 'C:/Users/james/Data/MIMIC/mimic-iii-clinical-database-1.4'\n","    save_root = 'C:/Users/james/Data/MIMIC/mimic-iii-chart-transformers'"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"cmfULdfNVGDk"},"source":["!pip install x_transformers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZiNstdm5rLcI"},"source":["## TENSORBOARD UTILS"]},{"cell_type":"code","metadata":{"id":"5RPe_7XfrPE3"},"source":["%reload_ext tensorboard"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HQIzy2pj3E9L"},"source":["from torch.utils.tensorboard import SummaryWriter\n","#writer = SummaryWriter()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mPGIdbjZro17"},"source":["Do writing! e.g. see [PyTorch tutorial](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html).\n","\n","Then `tensorboard --logdir runs`."]},{"cell_type":"code","metadata":{"id":"A9x-ZD4wLE3G"},"source":["tensorboard --logdir runs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ertC3K1Lfwi"},"source":["writer.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cg-gzA7tqjap"},"source":["## PRE-PROCESSING CODE"]},{"cell_type":"code","metadata":{"id":"iyvdkWonrunP"},"source":["import os\n","import numpy as np\n","import pandas as pd\n","import pickle as pickle\n","import torch\n","from sklearn.model_selection import train_test_split\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6YBDVsAsryy0","executionInfo":{"status":"aborted","timestamp":1625566930543,"user_tz":-60,"elapsed":7,"user":{"displayName":"James Cann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj37bBvXyKMzK890sou9cHG0PcfWI1S5-uihybe=s64","userId":"00456860292205766006"}}},"source":["# paths\n","\n","chartevents_path = os.path.join(data_root, \"CHARTEVENTS.csv\")\n","admissions_path = os.path.join(data_root,\"ADMISSIONS.csv\")\n","d_items_path = os.path.join(data_root, \"d_items.csv\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r_FdSrcoqa88","executionInfo":{"status":"aborted","timestamp":1625566930545,"user_tz":-60,"elapsed":8,"user":{"displayName":"James Cann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj37bBvXyKMzK890sou9cHG0PcfWI1S5-uihybe=s64","userId":"00456860292205766006"}}},"source":["# read in admissions\n","\n","admissions = pd.read_csv(admissions_path,\n","                         parse_dates=['ADMITTIME', 'DISCHTIME'])\n","\n","# extract only those charted and apply labelling logic\n","\n","charted = admissions[admissions.HAS_CHARTEVENTS_DATA == 1]\n","charted.drop('ROW_ID', axis=1, inplace=True)\n","charted['HADM_IN_SEQ'] = charted.groupby('SUBJECT_ID')['ADMITTIME'].rank().astype(int)\n","charted = charted.sort_values(by=['SUBJECT_ID', 'HADM_IN_SEQ'])\n","charted['ADMITTIME_NEXT'] = charted.groupby('SUBJECT_ID')['ADMITTIME'].shift(-1)\n","charted['DIS2ADM'] = charted['ADMITTIME_NEXT'] - charted['DISCHTIME']\n","charted['READM<7'] = (charted['DIS2ADM'] < pd.Timedelta(days=7)).astype(int)\n","charted['READM<30'] = (charted['DIS2ADM'] < pd.Timedelta(days=30)).astype(int)\n","charted.set_index('HADM_ID', inplace=True)\n","\n","# get hadm_ids for the first admission\n","\n","first_indices = charted[charted.HADM_IN_SEQ == 1].index.to_numpy()\n","\n","# split first-hadm_ids into train, val, test and check.\n","\n","train_indices, surplus = train_test_split(first_indices, train_size=0.8)\n","val_indices, test_indices = train_test_split(surplus, test_size=0.5)\n","del surplus\n","assert set(first_indices) == set(train_indices) | set(val_indices) | set(test_indices)\n","\n","# helpers\n","\n","\n","def ts_to_posix(time):\n","    return pd.Timestamp(time, unit='s').timestamp()\n","\n","\n","def get_admittime(hadm_id):\n","    time = charted.loc[hadm_id, 'ADMITTIME']\n","    return ts_to_posix(time)\n","\n","\n","def get_from_charted(hadm_id, label):\n","    return charted.loc[hadm_id, label]\n","\n","\n","# token mappings\n","\n","d_items = pd.read_csv(d_items_path)\n","\n","token_shift = 1\n","pad_token = 0\n","\n","itemid2token = dict(zip(d_items['ITEMID'], range(token_shift, token_shift + len(d_items))))\n","\n","# add special tokens to the dictionary\n","itemid2token['[PAD]'] = pad_token\n","#itemid2token['[BOS]'] = 1\n","#itemid2token['[EOS]'] = 2\n","#itemid2token['[SEP]'] = 3\n","\n","token2itemid = {v: k for k, v in itemid2token.items()}\n","token2label = dict(zip(range(len(d_items)), d_items['LABEL']))\n","\n","with open(os.path.join(save_root, 'mappings.pkl'), 'wb') as f:\n","    pickle.dump({'itemid2token': itemid2token,\n","                 'token2itemid': token2itemid},\n","                f)\n","\n","\n","def map2token(itemid):\n","    return itemid2token[np.int(itemid)]\n","\n","\n","def map2itemid(token):\n","    return str(token2itemid[token])\n","\n","\n","def map2itemidstr(tokens):\n","    return ' '.join(list(map(map2itemid, tokens)))\n","\n","\n","# loop through sets and generate output files\n","\n","for subset in ['val', 'train', 'test']:\n","    print(f'Processing {subset} set data...')\n","\n","    # grouper for charts\n","\n","    gpdf = (pd.read_csv(chartevents_path, skiprows=0, \n","                        nrows=10000000 if space != 'colab',\n","                        header=0,\n","                        usecols=['HADM_ID', 'CHARTTIME', 'ITEMID'],\n","                        dtype={'HADM_ID': np.int},\n","                        converters={'ITEMID': map2token},\n","                        parse_dates=['CHARTTIME'])\n","            .query(f'HADM_ID.isin(@{subset}_indices)')\n","            .groupby(by='HADM_ID')\n","            )\n","\n","    # initialise\n","\n","    tokens = dict()\n","    times = dict()\n","    times_rel = dict()\n","    labels = dict()\n","\n","    # populate with entries\n","\n","    for i in gpdf.groups:\n","        time_origin = get_admittime(i)\n","        temp = gpdf.get_group(i).sort_values(by=\"CHARTTIME\")\n","        tokens[i] = np.array(temp['ITEMID'], dtype=int)\n","        times[i] = np.fromiter(\n","            map(ts_to_posix, temp['CHARTTIME']),\n","            dtype=np.int64\n","        )\n","        times_rel[i] = times[i] - time_origin\n","        labels[i] = {\n","            'readm_7': get_from_charted(i, 'READM<7'),\n","            'readm_30': get_from_charted(i, 'READM<30')\n","        }\n","\n","    # write out charts to pickle\n","\n","    save_path = os.path.join(save_root, f'{subset}_charts.pkl')\n","\n","    with open(save_path, 'wb') as f:\n","        pickle.dump({f'{subset}_tokens': tokens,\n","                     f'{subset}_times': times,\n","                     f'{subset}_times_rel': times_rel}, f)\n","\n","    del tokens, times, times_rel, gpdf\n","\n","    # write out labels to pickle\n","\n","    save_path = os.path.join(save_root, f'{subset}_labels.pkl')\n","\n","    with open(save_path, 'wb') as f:\n","        pickle.dump({f'{subset}_labels': labels}, f)\n","\n","    del labels\n","\n","    print(f'{subset} set data processed!')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eAUowrZpu3ed"},"source":["## SELF-SUPERVISED MODE\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0YD8BurHdbDu"},"source":["### THE MODEL"]},{"cell_type":"code","metadata":{"id":"WUjIH_4q_gzL","executionInfo":{"status":"ok","timestamp":1625567724587,"user_tz":-60,"elapsed":565,"user":{"displayName":"James Cann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj37bBvXyKMzK890sou9cHG0PcfWI1S5-uihybe=s64","userId":"00456860292205766006"}}},"source":["import os\n","import copy\n","import tqdm\n","import random\n","\n","import numpy as np\n","import pandas as pd\n","import pickle as pickle\n","import torch\n","\n","from torch.utils.data import DataLoader, Dataset\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from x_transformers import TransformerWrapper, Decoder\n","from x_transformers.autoregressive_wrapper import AutoregressiveWrapper"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"myB-nKwoVmWe"},"source":["#### Mappings, Paths and Utils"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"RemqYJeTu2s7","executionInfo":{"status":"ok","timestamp":1625567749178,"user_tz":-60,"elapsed":6304,"user":{"displayName":"James Cann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj37bBvXyKMzK890sou9cHG0PcfWI1S5-uihybe=s64","userId":"00456860292205766006"}}},"source":["# paths\n","\n","train_path = os.path.join(data_root, \"train_charts.pkl\")\n","val_path = os.path.join(data_root, \"val_charts.pkl\")\n","mapping_path = os.path.join(data_root, \"mappings.pkl\")\n","\n","# misc\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# token mappings:  # TODO: refactor to module where possible.\n","\n","with open(mapping_path, 'rb') as f:\n","    mappings = pickle.load(f)\n","    itemid2token = mappings['itemid2token']\n","    token2itemid = mappings['token2itemid']\n","    del mappings\n","\n","num_tokens = len(itemid2token)\n","\n","# token mappings: decoders\n","\n","\n","def decode_token(token):\n","    return str(token2itemid[token])\n","\n","\n","def decode_tokens(tokens):\n","    return ' '.join(list(map(decode_token, tokens)))\n","\n","\n","# get data\n","\n","def fetch_data(path, var_key):\n","    with open(path, 'rb') as f:\n","        data = pickle.load(f)\n","    return data[var_key]\n","\n","\n","trX = fetch_data(train_path, 'train_tokens')\n","vaX = fetch_data(val_path, 'val_tokens')\n","\n","data_train = {k: torch.from_numpy(v) for k, v in trX.items()}\n","data_val = {k: torch.from_numpy(v) for k, v in vaX.items()}\n","\n","\n","# yield from loader\n","\n","def cycle(loader):\n","    while True:\n","        for data in loader:\n","            yield data\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FgHrfl-GVe0l"},"source":["#### Constants & Model"]},{"cell_type":"code","metadata":{"id":"6GVoPb-bVdw5"},"source":["# constants & hyperparameters \n","\n","NUM_EPOCHS = 10\n","NUM_BATCHES = 1000\n","BATCH_SIZE = 4\n","GRADIENT_ACCUMULATE_EVERY = 4  # 4\n","LEARNING_RATE = 1e-4\n","VALIDATE_EVERY = 100\n","CHECKPOINT_AFTER = 100\n","GENERATE_EVERY = 100\n","GENERATE_LENGTH = 100\n","SEQ_LEN = 200\n","\n","# instantiate GPT-like decoder model\n","\n","\n","LAYER_SPEC = {'dim':100, 'depth':3, 'heads':4}  # full: 512, 6, 8\n","\n","model = TransformerWrapper(\n","    num_tokens=num_tokens,  # Expects each val in data to be [0, num_tokens)\n","    max_seq_len=SEQ_LEN, \n","    attn_layers=Decoder(\n","        dim=LAYER_SPEC['dim'],\n","        depth=LAYER_SPEC['depth'],\n","        heads=LAYER_SPEC['heads'])\n",")\n","\n","pre_model = AutoregressiveWrapper(model)\n","pre_model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IIMyYHwrUxST"},"source":["#### Datasets & Dataloaders"]},{"cell_type":"code","metadata":{"id":"fwNfiD4tUwFe","executionInfo":{"status":"ok","timestamp":1625567846452,"user_tz":-60,"elapsed":196,"user":{"displayName":"James Cann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj37bBvXyKMzK890sou9cHG0PcfWI1S5-uihybe=s64","userId":"00456860292205766006"}}},"source":["class ClsSamplerDataset(Dataset):  # TODO: tidy __getitem__ method with more natural pad operations.\n","    def __init__(self, data, seq_len, labels=None):\n","        super().__init__()\n","        self.data = data\n","        self.labels = labels\n","        self.seq_len = seq_len\n","        self.lookup = dict(zip(np.arange(len(self.data)), self.data.keys()))\n","\n","    def __getitem__(self, key):  # a.t.m. when data[key] shorter length than SEQ_LEN, padded with 0.\n","        index = self.lookup[key]\n","        item_len = self.data[index].size(0)\n","        rand_start = torch.randint(0, item_len - self.seq_len, (1,)) if item_len > self.seq_len else 0\n","        lenfromseq = min(item_len, self.seq_len)\n","        sample = torch.zeros(self.seq_len)\n","        sample[:lenfromseq] = self.data[index][rand_start: rand_start + lenfromseq]\n","\n","        if self.labels is not None:\n","            label = torch.tensor(self.labels[index])\n","            return sample.long().to(device), label.long().to(device)\n","        else:\n","            return sample.long().to(device)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","\n","train_dataset = ClsSamplerDataset(data_train, SEQ_LEN)\n","val_dataset   = ClsSamplerDataset(data_val, SEQ_LEN)\n","\n","train_loader  = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n","val_loader    = DataLoader(val_dataset,   batch_size=BATCH_SIZE)\n","\n","train_cycler  = cycle(train_loader)\n","val_cycler    = cycle(val_loader)\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qoyiGyrsfZ29"},"source":["### TRAINING LOOP"]},{"cell_type":"code","metadata":{"id":"_4s0dfbsfW3f"},"source":["optim = torch.optim.Adam(pre_model.parameters(), lr=LEARNING_RATE)\n","ckpt_path = os.path.join(save_root, \"pre_model_exp1.pt\")\n","\n","writer = SummaryWriter(\n","    log_dir=\"runs/pre_model\",\n","    filename_suffix='_' + '_'.join(map(str, LAYER_SPEC.values()))\n","    )\n","\n","# training loop\n","\n","best_val_loss = np.inf\n","\n","for epoch in range(NUM_EPOCHS):\n","  for i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10.,\n","                     desc=f'epoch {epoch}:', colour='green'):\n","      pre_model.train()\n","\n","      for __ in range(GRADIENT_ACCUMULATE_EVERY):\n","          loss = pre_model(next(train_loader))\n","          loss.backward()\n","\n","      torch.nn.utils.clip_grad_norm_(pre_model.parameters(), 0.5)\n","      optim.step()\n","      optim.zero_grad()\n","\n","      # parameter tracking\n","\n","      writer.add_scalar('train_loss', loss.item(),\n","                        epoch * NUM_BATCHES + i\n","                        )\n","\n","      # validate model\n","\n","      if i % VALIDATE_EVERY == 0:\n","          pre_model.eval()\n","          with torch.no_grad():\n","              val_loss = pre_model(next(val_loader)).item()\n","              \n","              writer.add_scalar('val_loss', val_loss,\n","                        epoch * NUM_BATCHES + i\n","                        )\n","\n","              if val_loss < best_val_loss:\n","                  print(f'VL: {val_loss} < BVL: {best_val_loss}')\n","                  best_val_loss = val_loss\n","\n","                  # checkpoint model\n","\n","                  if i > CHECKPOINT_AFTER:\n","                    print(\"Checkpoint saving...\")\n","                    torch.save({\n","                        'train_step': i,\n","                        'model_state_dict': pre_model.state_dict(),\n","                        'LAYER_SPEC': LAYER_SPEC,\n","                        'SEQ_LEN': SEQ_LEN,\n","                        'optim_state_dict': optim.state_dict(),\n","                        'val_loss': val_loss\n","                    }, ckpt_path)\n","                    print(\"Checkpoint saved!\\n\")\n","      \n","      # generate sequence\n","\n","      if i % GENERATE_EVERY == 0:\n","          pre_model.eval()\n","          inp = random.choice(val_dataset)[:-1]\n","          primer_str = decode_tokens(inp.cpu().numpy())\n","          print('\\nprimer:', primer_str, '*' * 100, sep='\\n')\n","\n","          sample = pre_model.generate(inp, GENERATE_LENGTH)\n","          sample_str = decode_tokens(sample.cpu().numpy())\n","          print('output:', sample_str, '\\n', sep='\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MVUY6qYzkdOu"},"source":["### EVALUATE"]},{"cell_type":"code","metadata":{"id":"Tm9Q6o0zkcfp","executionInfo":{"status":"ok","timestamp":1625568172904,"user_tz":-60,"elapsed":203,"user":{"displayName":"James Cann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj37bBvXyKMzK890sou9cHG0PcfWI1S5-uihybe=s64","userId":"00456860292205766006"}}},"source":["@torch.no_grad()\n","def evaluate(model, dataloader):\n","    model.eval()\n","    cum_loss = 0\n","    counter = 0\n","    for batch in dataloader:\n","        counter += 1\n","        batch_size = batch.shape[0]\n","        val_loss = model(batch).item()\n","        cum_loss += val_loss\n","    avg_loss = cum_loss/(batch_size*counter)\n","    return cum_loss, avg_loss"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QNTfwTDxlNgA","executionInfo":{"status":"ok","timestamp":1625506895767,"user_tz":-60,"elapsed":93489,"user":{"displayName":"James Cann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj37bBvXyKMzK890sou9cHG0PcfWI1S5-uihybe=s64","userId":"00456860292205766006"}},"outputId":"9233365e-9b4b-4492-ebfa-b1aa88e181be"},"source":["evaluate(pre_model, val_loader)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2384.7820653915405, 2.3943595034051612)"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"L9kSueJZWbW9"},"source":["Will this code work for finetuning too?"]},{"cell_type":"markdown","metadata":{"id":"bllR8BhQdfpH"},"source":["### GENERATING SEQUENCES"]},{"cell_type":"code","metadata":{"id":"pqGhUlnRGWcM","executionInfo":{"status":"ok","timestamp":1625568000241,"user_tz":-60,"elapsed":848,"user":{"displayName":"James Cann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj37bBvXyKMzK890sou9cHG0PcfWI1S5-uihybe=s64","userId":"00456860292205766006"}}},"source":["# reading d_items for interpretability \n","d_items_path = os.path.join(data_root, \"d_items.csv\")\n","d_items = pd.read_csv(d_items_path, index_col='ITEMID', dtype={'ITEMID': str})"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"H3gZplilK-EL","executionInfo":{"status":"ok","timestamp":1625568001731,"user_tz":-60,"elapsed":214,"user":{"displayName":"James Cann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj37bBvXyKMzK890sou9cHG0PcfWI1S5-uihybe=s64","userId":"00456860292205766006"}}},"source":["def decode_token(token):\n","    return str(token2itemid[token])\n","\n","def decode_tokens(tokens):\n","    return ' '.join(list(map(decode_token, tokens)))\n","\n","def token2label(token):\n","    if token == 0:\n","        return '[PAD]'\n","    else:\n","        itemid = token2itemid[token]\n","        x = d_items.loc[itemid, 'LABEL']\n","    return x\n","\n","def tokens2labels(tokens):\n","    return '\\n\\t -> '.join(list(map(token2label, tokens)))"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"cizrIjWVdwIX","executionInfo":{"status":"ok","timestamp":1625567975094,"user_tz":-60,"elapsed":3344,"user":{"displayName":"James Cann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj37bBvXyKMzK890sou9cHG0PcfWI1S5-uihybe=s64","userId":"00456860292205766006"}}},"source":["# fetch and load model state_dict\n","\n","weights_path = os.path.join(data_root, 'models', 'pre_model_exp1.pt')\n","X = torch.load(weights_path, map_location=device)\n","states = X['model_state_dict']\n","base_states = { k[len('net.'):] if k[:len('net.')] == 'net.' else k : v for k, v in states.items()}\n","\n","pre_model.load_state_dict(states)\n","pre_model.to(device)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"xjk16p9KeyUv"},"source":["pre_model.eval()\n","with torch.no_grad():\n","    prompt = random.choice(val_dataset)[0:18]\n","    #prompt = torch.cat((prompt, torch.tensor([0]).to(device)))\n","\n","    sample = pre_model.generate(start_tokens=prompt, seq_len = 50, eos_token=0)\n","    print(\"prompt:\\t\", tokens2labels(prompt.cpu().numpy()))\n","    print(\"model:\\t\", tokens2labels(sample.cpu().numpy()), '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Da2PT5WVaEBf","executionInfo":{"status":"ok","timestamp":1625568213097,"user_tz":-60,"elapsed":35242,"user":{"displayName":"James Cann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj37bBvXyKMzK890sou9cHG0PcfWI1S5-uihybe=s64","userId":"00456860292205766006"}},"outputId":"e7b4b0a2-582e-42ad-bc43-2931dea599ff"},"source":["evaluate(pre_model, train_loader)"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3122.3268125355244, 1.5635086692716698)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"YYCK4nwgWlSu"},"source":["### TESTING - DO NOT USE"]},{"cell_type":"code","metadata":{"id":"w47h1Q6_Wnz_","executionInfo":{"status":"ok","timestamp":1625569500434,"user_tz":-60,"elapsed":211,"user":{"displayName":"James Cann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj37bBvXyKMzK890sou9cHG0PcfWI1S5-uihybe=s64","userId":"00456860292205766006"}}},"source":["test_path    = os.path.join(data_root, 'test_charts.pkl')\n","tsX          = fetch_data(test_path, 'test_tokens')\n","data_test    = {k: torch.from_numpy(v) for k, v in tsX.items()}\n","test_dataset = ClsSamplerDataset(data_test, SEQ_LEN)\n","test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n","test_cycler  = cycle(test_loader)"],"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wVDVCWE8vnWZ"},"source":["## FINE-TUNING MODE"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"M4oE0duaUQIJ"},"source":["class FinetuningWrapper(nn.Module):\n","    def __init__(self, net, num_classes, state_dict = None,\n","                 ignore_index = -100, pad_value = 0):\n","        super().__init__()\n","        self.pad_value = pad_value\n","        self.ignore_index = ignore_index\n","        self.num_classes = num_classes\n","\n","        self.net = copy.deepcopy(net)  # deepcopy is necessary here.\n","        self.max_seq_len = self.net.max_seq_len\n","\n","        # initialise net from pretrained\n","        if state_dict is not None:\n","            self.net.load_state_dict(state_dict)\n","\n","        # define classifier head layers\n","        self.num_features = net.to_logits.in_features * 200\n","        self.net.clf1 = nn.Linear(self.num_features, num_classes, bias=True)\n","\n","    def forward(self, X, Y, **kwargs):\n","        Z = self.net(X, return_embeddings=True, **kwargs)\n","        Z = torch.flatten(Z, start_dim=1)\n","        logits = self.net.clf1(Z)\n","        loss = F.cross_entropy(logits, Y)\n","        return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"5a_vWVFHUQIM"},"source":["train_lbl_path = os.path.join(data_root, \"train_labels.pkl\")\n","val_lbl_path = os.path.join(data_root, \"val_labels.pkl\")\n","FT_BATCH_SIZE = 100\n","\n","# fetch labels\n","with open(train_lbl_path, 'rb') as f:\n","    X = pickle.load(f)\n","    train_labels = {k: v['readm_30'] for k, v in  X['train_labels'].items()}\n","    del X\n","\n","with open(val_lbl_path, 'rb') as f:\n","    X = pickle.load(f)\n","    val_labels = {k: v['readm_30'] for k, v in  X['val_labels'].items()}\n","    del X\n","\n","# generate datasets and loaders\n","ft_train_dataset = ClsSamplerDataset(data_train, SEQ_LEN, labels=train_labels)\n","ft_val_dataset = ClsSamplerDataset(data_val, SEQ_LEN, labels=val_labels)\n","\n","ft_train_loader = cycle(DataLoader(ft_train_dataset, batch_size=FT_BATCH_SIZE))\n","ft_val_loader = cycle(DataLoader(ft_val_dataset, batch_size=FT_BATCH_SIZE))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"ggZs5eGAUQIN"},"source":["# fetch model weights\n","weights_path = os.path.join(data_root, 'pre_model.pt')\n","X = torch.load(weights_path)\n","states = X['model_state_dict']\n","base_states = { k[len('net.'):] if k[:len('net.')] == 'net.' else k : v for k, v in states.items()}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"zc4WuVlZUQIN"},"source":["### FINETUNING LOOP"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"1uqkNxB7UQIN"},"source":["# initialisation\n","\n","model_ft = FinetuningWrapper(model, num_classes=2, state_dict=base_states)\n","model_ft.to(device)\n","\n","optim_ft = torch.optim.Adam(model_ft.parameters(), lr=0.001)\n","ckpt_ft_path = os.path.join(save_root, \"models\", \"model_ft2.pt\")\n","logs_path = os.path.join(save_root, \"logs\", \"model_ft2\")\n","\n","NUM_FT_BATCHES = 100\n","\n","writer = SummaryWriter(logs_path)\n","\n","# training loop\n","\n","best_val_loss = np.inf\n","for i in tqdm.tqdm(range(NUM_FT_BATCHES), mininterval=10., desc='fine-tuning'):\n","    model_ft.train()\n","\n","    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n","        X, Y = next(ft_train_loader)\n","        loss = model_ft(X, Y)\n","        loss.backward()\n","      \n","    writer.add_scalar('loss', loss.item(),\n","                      i)\n","\n","    print(f'training loss: {loss.item()}')\n","    torch.nn.utils.clip_grad_norm_(model_ft.parameters(), 0.5)\n","    optim_ft.step()\n","    optim_ft.zero_grad()\n","\n","    # validate model_ft\n","\n","    if i % VALIDATE_EVERY == 0:\n","        model_ft.eval()\n","        with torch.no_grad():\n","            X, Y = next(ft_val_loader)\n","            val_loss = model_ft(X, Y)\n","            print(f'validation loss: {val_loss.item()}')\n","            if val_loss.item() < best_val_loss:\n","                best_val_loss = val_loss.item()\n","\n","        if (i > CHECKPOINT_AFTER) & (val_loss.item() < best_val_loss):\n","                torch.save({\n","                    'train_step': i,\n","                    'model_state_dict': model_ft.state_dict(),\n","                    'SEQ_LEN': SEQ_LEN,\n","                    'optim_state_dict': optim_ft.state_dict(),\n","                    'val_loss': val_loss\n","                }, ckpt_ft_path)\n","                print(\"Checkpoint saved!\\n\")\n","\n","writer.close()\n","\n","    # generate sequence\n","\n","    #if i % GENERATE_EVERY == 0:\n","    #    model.eval()\n","    #    inp = random.choice(val_dataset)[:-1]\n","    #    primer_str = decode_tokens(inp.numpy())\n","    #    print('primer:', primer_str, '*' * 100, sep='\\n')\n","    #\n","    #    sample = model.generate(inp, GENERATE_LENGTH)\n","    #    sample_str = decode_tokens(sample.numpy())\n","    #    print('output:', sample_str, sep='\\n')"],"execution_count":null,"outputs":[]}]}